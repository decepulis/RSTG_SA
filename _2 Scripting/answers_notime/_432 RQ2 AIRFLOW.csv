body,Asker,Answered,Answered by
[~bolke] Any though on that this (the corresponding PR is https://github.com/apache/incubator-airflow/pull/2444)  We are trying to switch from Oozie to Airflow at Booking.com and would really need this feature (or maybe there's already a way to achieve this) for us to go forward with the migration.,Raphael Lopez kaufman,,
"[~bolke] thanks for your feedback, would you have time to have a quick look at this ticket too: https://issues.apache.org/jira/browse/AIRFLOW-1398?",Raphael Lopez kaufman,,
"I'm particularly interested in thoughts from [~alexvanboxel], [~wwlian], and [~fenglu]",Peter Dolan,1,"Feng Lu, Wilson Lian"
"SGTM, maybe we can include other standardization efforts on GCP operators as well (see https://issues.apache.org/jira/browse/AIRFLOW-820 )? Let's also make sure to do it in a backwards-compatible way and not break any existing DAGs that depend on an older version of these operators.",Feng Lu,1,Peter Dolan
[~bolke] Any though on that this (the corresponding PR is https://github.com/apache/incubator-airflow/pull/2431)  We are trying to switch from Oozie to Airflow at Booking.com and would really need this feature (or maybe there's already a way to achieve this) for us to go forward with the migration.,Raphael Lopez kaufman,,
"[~bolke] I implemented your suggestion, would you mind reviewing?",Raphael Lopez kaufman,,
"To be clear, are you planning to REMOVE support for S3/GCS? We are heavy users of GCS for logging, and this would be a show stopper for us.",Chris Riccomini,1,Allison Wang & Chris Riccomini
"It was exactly the same sort of thinking on my side.     Good point about the logs. It was sort of design decision as we capture all logs centrally in ELK so we had no need to stream the logs to airflow as such. It is not hard to add though.     Also regarding your comment that it would be more powerful if you can supply a pod definition directly - thats another decision to hide the pod definition as such to make deployment of simple jobs simpler. I have a version where you can give a pod definition instead in case you need to do something complex, but for most jobs we have we just have an image (often with default command) and we dont want to force people to learn yet another library just to run a simple command.     Do you mind collaborating to get out something nice and useful to everybody?",Michal Dziemianko,1,Dennis Doctor
"[~aoen], changing this to blocker for 1.8.1. LMK if you don't think we should block the release on it.",Chris Riccomini,1,Dan Davydov
Was this confirmed yet?,Bolke de Bruin,1,Dan Davydov 
It seems like the commit was reverted but I'm not sure if testing was done: https://github.com/apache/incubator-airflow/pull/2195? If not I can try to take a look soon but just wanted to confirm first [~bolke],Dan Davyov,,
Please review my Pull Request #2149:  https://github.com/apache/incubator-airflow/pull/2149,Chris Sng,1,Jeremiah Lowin
verify_integrity in models.py is where you would like to look at. As mentioned it sets a state to REMOVED if a task instance does not have a corresponding task in a DAG. It is there for auditing and versioning reasons. Does it affect running anything? A changed DAG cannot have a dependency on a non existent task.    Ie. please explain why this is a blocker.,Bolke de Bruin,1,Dan Davydov
"Command:  {quote}airflow backfill teztg -t create_and_transfer_roster -s 2017-06-14 -e 2017-06-14{quote}    DAG:  {code}  import airflow  from airflow.executors import SequentialExecutor  from airflow.operators.dummy_operator import DummyOperator  from airflow.operators.python_operator import PythonOperator  from airflow.operators.subdag_operator import SubDagOperator  from airflow.models import DAG  from datetime import datetime, timedelta    default_args = {      'owner': 'airflow',      'start_date': datetime(2016, 3, 15),  }    dag = DAG('teztg', default_args=default_args)      subdag = DAG('teztg.create_and_transfer_roster', default_args=default_args)    subdag_operator = SubDagOperator(task_id='create_and_transfer_roster',                                   subdag=subdag,                                   executor=SequentialExecutor(),                                   dag=dag)    def create_roster(ds, macros, **kwargs):      pass      PythonOperator(      task_id='create_roster',      python_callable=create_roster,      dag=subdag,      provide_context=True)    first_task = DummyOperator(task_id=""first_task"", dag=dag)  {code}    Expected:  first_task TI does not change (stays as none)  create_and_transfer_roster TI is run    Actual:  first_task TI gets its state set to removed  create_and_transfer_roster TI is not run, backfill errors with airflow.exceptions.AirflowException: Task first_task not found    [~bolke] if you think these are related to your changes (addition of task.REMOVED state/backfill changes) do you mind taking a look? Otherwise let me know and I can do so.",Dan Davydov,1,Bolke de Bruin
"I just ran this on my AIRFLOW-910 branch and I am not seeing this behaviour. I did change your execution date to 2016 as 2017 won't run.    {code}  MariaDB [airflow]> select * from task_instance;  +----------------------------+----------------------------------+----------------------------+----------------------------+----------------------------+----------+---------+------------+--------------------------+----------+--------+------+---------+-----------------+----------------+-------------+-------+  | task_id                    | dag_id                           | execution_date             | start_date                 | end_date                   | duration | state   | try_number | hostname                 | unixname | job_id | pool | queue   | priority_weight | operator       | queued_dttm | pid   |  +----------------------------+----------------------------------+----------------------------+----------------------------+----------------------------+----------+---------+------------+--------------------------+----------+--------+------+---------+-----------------+----------------+-------------+-------+  | create_and_transfer_roster | teztg                            | 2016-06-14 00:00:00.000000 | 2017-03-02 08:58:35.867219 | 2017-03-02 08:58:46.868470 |  11.0013 | success |          1 | bolkes-macbook-pro.local | bolke    |      2 | NULL | default |               1 | SubDagOperator | NULL        | 46856 |  | create_roster              | teztg.create_and_transfer_roster | 2016-06-14 00:00:00.000000 | 2017-03-02 08:58:42.681159 | 2017-03-02 08:58:42.702009 |  0.02085 | success |          1 | bolkes-macbook-pro.local | bolke    |      4 | NULL | default |               1 | PythonOperator | NULL        | 46862 |  +----------------------------+----------------------------------+----------------------------+----------------------------+----------------------------+----------+---------+------------+--------------------------+----------+--------+------+---------+-----------------+----------------+-------------+-------+  {code}      {code}  [2017-03-02 08:58:29,039] {jobs.py:1756} INFO - Checking run <DagRun teztg @ 2016-06-14 00:00:00: backfill_2016-06-14T00:00:00, externally triggered: False>  [2017-03-02 08:58:29,044] {models.py:1125} INFO - Dependencies all met for <TaskInstance: teztg.create_and_transfer_roster 2016-06-14 00:00:00 [None]>  [2017-03-02 08:58:29,047] {base_executor.py:50} INFO - Adding to queue: airflow run teztg create_and_transfer_roster 2016-06-14T00:00:00 --local -sd DAGS_FOLDER/task_removed_bug.py  [2017-03-02 08:58:33,985] {jobs.py:1949} INFO - [backfill progress] | dag run 1 of 1 | tasks waiting: 1 | succeeded: 0 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0  [2017-03-02 08:58:33,987] {local_executor.py:45} INFO - LocalWorker running airflow run teztg create_and_transfer_roster 2016-06-14T00:00:00 --local -sd DAGS_FOLDER/task_removed_bug.py  [2017-03-02 08:58:33,989] {models.py:1125} INFO - Dependencies all met for <TaskInstance: teztg.create_and_transfer_roster 2016-06-14 00:00:00 [scheduled]>  /Users/bolke/Documents/dev/airflow_env/lib/python2.7/site-packages/airflow-1.9.0.dev0+apache.incubating-py2.7.egg/airflow/configuration.py:548: DeprecationWarning: This method will be removed in future versions.  Use 'parser.read_file()' instead.    self.readfp(StringIO.StringIO(string))  [2017-03-02 08:58:34,561] {__init__.py:56} INFO - Using executor LocalExecutor  Logging into: /Users/bolke/airflow/logs/teztg/create_and_transfer_roster/2016-06-14T00:00:00  [2017-03-02 08:58:38,987] {jobs.py:1949} INFO - [backfill progress] | dag run 1 of 1 | tasks waiting: 1 | succeeded: 0 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0  [2017-03-02 08:58:38,990] {models.py:1119} INFO - Dependencies not met for <TaskInstance: teztg.create_and_transfer_roster 2016-06-14 00:00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state which is not a valid state for execution. The task must be cleared in order to be run.  [2017-03-02 08:58:38,990] {models.py:1119} INFO - Dependencies not met for <TaskInstance: teztg.create_and_transfer_roster 2016-06-14 00:00:00 [running]>, dependency 'Task Instance Not Already Running' FAILED: Task is already running, it started on 2017-03-02 08:58:35.867219.  [2017-03-02 08:58:43,994] {jobs.py:1949} INFO - [backfill progress] | dag run 1 of 1 | tasks waiting: 1 | succeeded: 0 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0  [2017-03-02 08:58:43,997] {models.py:1119} INFO - Dependencies not met for <TaskInstance: teztg.create_and_transfer_roster 2016-06-14 00:00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state which is not a valid state for execution. The task must be cleared in order to be run.  [2017-03-02 08:58:43,997] {models.py:1119} INFO - Dependencies not met for <TaskInstance: teztg.create_and_transfer_roster 2016-06-14 00:00:00 [running]>, dependency 'Task Instance Not Already Running' FAILED: Task is already running, it started on 2017-03-02 08:58:35.867219.  [2017-03-02 08:58:49,001] {jobs.py:1949} INFO - [backfill progress] | dag run 1 of 1 | tasks waiting: 1 | succeeded: 0 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0  [2017-03-02 08:58:54,007] {jobs.py:1849} WARNING - (u'teztg', u'create_and_transfer_roster', datetime.datetime(2016, 6, 14, 0, 0)) state success not in tasks_to_run=[]  [2017-03-02 08:58:54,007] {jobs.py:1949} INFO - [backfill progress] | dag run 1 of 1 | tasks waiting: 0 | succeeded: 1 | kicked_off: 1 | failed: 0 | skipped: 0 | deadlocked: 0  [2017-03-02 08:58:54,009] {models.py:3984} INFO - Updating state for <DagRun teztg @ 2016-06-14 00:00:00: backfill_2016-06-14T00:00:00, externally triggered: False> considering 1 task(s)  [2017-03-02 08:58:54,011] {models.py:4030} INFO - Marking run <DagRun teztg @ 2016-06-14 00:00:00: backfill_2016-06-14T00:00:00, externally triggered: False> successful  [2017-03-02 08:58:54,028] {jobs.py:2001} INFO - Backfill done. Exiting.  {code}    Can this be related to your local patches?",Bolke de Bruin,1,Dan Davydov
"Ah I think I know why it's not reproducing for you. Can you try letting the scheduler run a dagrun, and then backfill the dagrun once it completes (same DAG)?    so e.g. let it run for one dagrun via scheduler (might need to unpause it if it's paused) then   airflow backfill teztg -t create_and_transfer_roster -s 2016-03-15 -e 2016-03-15",Dan Davydov,1,Bolke de Bruin
"Issue resides in cli.py and will only happen when a specific task is used:    {code}      if args.task_regex:          dag = dag.sub_dag(              task_regex=args.task_regex,              include_upstream=not args.ignore_dependencies)  {code}    This creates a subset of the tasks from a dag_run, with the same name as the original. Hence it will set a task to removed if you verify the integrity of a dag run, cause that will consider the tree of the original (whole).    get_task_instances picks up all instances, including ""removed' ones from the original (whole) dag and this is not filtered in the backfill. Hence the lists mismatch and an AirflowException is thrown.    The quick and dirty fix is to not mark removed if run from backfill and filter the list of ""get_task_instances"".     However the functionality if sub_dag is awkward imho - and seems not to be tested, and might need a real fix. What do you think?",Bolke de Bruin,1,Dan Davydov
PR is available with unit tests [~aoen]. Please review,Bolke de Bruin,,
"[~bolke] I added a bunch of manual logging to verify this. What seems to happen (in my simple gist example) is DAG starts with 4 tasks. The multiple scheduler processes kick them off (the jobs.py:1095) because at that time they are all runnable (since none are yet running). That models.py:1291 does another check to make sure it is actually runnable (2 of them are, but then due to the dag concurrency of 2, the third and fourth one aren't). The 2 tasks stay marked as queued but never actually get picked up by the scheduler again (unless you restart it). Is there some other state that that models.py:1291 check could move the tasks to if that last minute check found they aren't actually runnable? (I found by making them State.NONE it worked, but that seems hackish as it keeps bouncing back and forth between QUEUED and NONE until it can actually run)",Vijay Krishma Ramesh,1,Bolke de Bruin
"[~aoen] It does propagate in its current form: if you mark success a ""subdag"" operator from its parent dag it will mark success all tasks part of that subdag. My PR just verifies the tree of the dag to make sure dag runs are created for any missing subdag dag runs. I consider this to be something of a hack as it is in my opinion the wrong place to do it (the webserver).    BTW: Also the unit tests confirm propagation. So are we talking about the same thing?",Bolke de Bruin,1,Dan Davydov
Can i take this feature ?,Phawin Khongkhasawan,,
[~aoen] Would you give me an instruction ?,Phawin Khongkhasawan,1,Dan Davydov
Any reason why you are suggesting only adding the day like that? It isn't very intuitive to read and could be misunderstood as some time between 21:29 and 23:29.    I would suggest keeping the same time format as we use everywhere else YYYY-MM-DD HH:MM:SS,Amit Pahwa,1,Dan Davydov
Are the example endpoints just the two in the docs? Specifically:    /api/experimental/dags/<DAG_ID>/tasks/<TASK_ID> returns info for a task (GET).  /api/experimental/dags/<DAG_ID>/dag_runs creates a dag_run for a given dag id (POST).    Or is there a roadmap somewhere?,James Santucci,,
"Hi,    I see that 1.8 has finally been tagged, is there any plan to release it to pypi ? How is that automated/processed ?    Cheers",Ultrabug,1,Arthur Wiedmer
"BTW [~artwr], would you please be kind enough to point me where this vote / release tracking is taking place ? Looks like I'm lost on JIRA...",Ultrabug,1,Bolke de Bruin
I see this has a closing PR. Is this ticket still relevant or can it be closed?,Alex Van Boxel,1,Alex Guziel
"Not sure if you saw the note on the related issue, but perhaps this could  be implemented via a kwargs passed through to the API, so that one-off  feature changes don't require end-to-end plumbing in the future?",Sam McVeety,1,Chris Riccomini
"I agree re: kwargs, but I'd rather keep it as a separate thing from this just for simplicity's sake. Could you open a second JIRA to use kwargs for Google hook/operator params?",Chris Riccomini,1,Sam McVeety
"[~criccomini]: athough this ticket is closed (can you add a ""gcp"" label on this ticket), you're probably an admin. Thanks.",Alex Van Boxel,1,Chris Riccomini
"What's wrong with workaround 2?    The point of parallelism is to control how many processes are running. A sensor that's waiting is in a while loop eating processing time, so you don't really want to ignore them for parallelism's sake.    Short timeout, with tuned retry delay, and retry counts, will work just fine.",George Leslie Waksman,,
"Porting convo from gitter to ticket:     {code}  [kyuen]  option 2 is a valid way of preventing the problem. Just created the issue to see what people think the sensors should do. Didn't want to make a pr before seeing how people feel about how the sensors should behave    [George Leslie-Waksman]  Sensor doesn't really give the best sense for how they work. I think @mistercrunch was talking about more complex operations in the scheduler as an alternative way of doing things, but I think that's still a work in progress.  {code}    Would love to hear more from others on how they'd like sensors to behave",Kevin Yuen,1,Laura Lorenz
[~criccomini] any thoughts on this?,Rob Froetscher,,
Can I take this issue?,Yongjun Park,1,Jason DeCorte
"This is for GCS to BQ operator, right?",Chris Riccomini,1,Jeremiah Lowin
"Where in the docs would you place this? Submit a PR if you think this is a necessary feature. For example, if someone were to require this link in the natural process of reading through a getting-started guide, I'd say it makes sense. Otherwise, i'd expect folks who are working on PRs to use Google to look into Flask-Admin, SQLAlchemy, or someone other 3rd party framework that Airflow leverages.",Siddharth Anand,1,Russell Pierce
[~jlowin][~criccomini] What do you think?,Siddharth Anand,,
"[~mtdewulf] I'm considering taking ownership of this issue since you mention on your PR:    ""Since I've posted this PR, my team has decided that Airflow is not appropriate for our use case, so I probably won't be able to spend any cycles updating it.""    I assume you're happy for me to assign this to myself and take it over?",Robin Miller,1,Michael DeWulf
"""=+1    [~maxime.beauchemin@apache.org],    {noformat}  While we're [decreasingly] comfortable installing all sorts of packages (R packages, Python libs, related apt packages, ...)   on Airflow workers, we clearly don't want any of that on the Yarn boxes directly.{noformat}    It could be easier to implement once Docker on YARN infrastructure is complete?   YARN-3611, YARN-4793     https://hadoop.apache.org/docs/r2.7.2/hadoop-yarn/hadoop-yarn-site/DockerContainerExecutor.html""",Ruslan Dautkhanov,,
"> The task can monitor the time since last heartbeat and kill itself to prevent such cases.    In your example, if the task can't get access to the DB how does it know? Are you planning on keeping some local in-mem variable that tracks the last successful heartbeat write?",Chris Riccomini,1,Paul Yang
Could you add some more description to this ticket? I don't understand it.,Chris Riccomini,1,Tongzhou Wang
"Also, should I track other unreferenced dependencies? I'm sure there's a couple of others lying around. If so, should I open an another ticket/PR to cover this extended work?",Kevin Deldycke,,
"Ah, nice catch. Can you send a pull request, please?",Chris Riccomini,1,peter pang
"I’m a little concerned about performance. The dag runs page, for example, barely loads for us, since we have 100s of DAGs at a 15m interval.  Perhaps recent DAG runs is better?",Chris Riccomini,1,siddarth anand
"Isn't this possible with an ExternalTaskSensor(same_dag_id, previous_task_id, allowed_states=[State.SUCCESS, State.FAILED])?",Matthis Huschle,,
Can you try running off of master? I'm wondering if my PR in AIRFLOW-200 fixes this.,Chris Riccomini,1,dud
Thoughts on support for this?  It would be nice to see this at the DAG level so individual DAGs could be triggered in a timezone aware way by the the scheduler.,David Klosowski,,
"Considering https://issues.apache.org/jira/browse/AIRFLOW-758, would it not be more straight forward to just revert back to execpv and run gunicorn with the --reload argument? Granted, this functionality is listed under the ""Debugging"" section of gunicorn's settings documentation, but it might be still be appropriate for this use case. Link to gunicorn docs: http://docs.gunicorn.org/en/19.0/settings.html#reload",Chad T Henderson,1,Li Xuanji
"[~bolke], you committed the PR for this, right?",Chris Riccomini,1,ASK subversion and git services
Makes sense to me. Want to send a PR?,Chris Riccomini,,
"Hey guys - this seems like the closest thing to an issue I'm seeing.    I have a custom executor and a custom operator as a plugin.  If I specify my custom executor in the config, airflow takes a different path than if I don't specify using the custom executor.  If I'm configured with a built in executor like LocalExecutor, the plugin loads fine.  If I'm configured to use my custom executor, then it loads the plugins which also loads the modules my custom operator is defined in, which is normal.  I'm subclassing BaseSensorOperator and as its importing modules it throws an error about not being able to import BaseOperator from airflow.models from airflow/operators/__init__.py.    Long story, any ideas on how to tackle this?  Would be glad to work on a PR with a bit of guidance.  Thanks!",Greg Neiheisel,,
This is really awesome analysis. Can you send a PR? (unless I missed it),Chris Riccomini,1,dud
"[~sanand], not sure what the protocol is for creating issues related to something I'd like to implement. Is what I've done here acceptable?",Rob Froetscher,1,Chris Riccomini
"[~rfroetscher], are you still planning to work on this? If not, we can re-assign to [~gjreda].",Chris Riccomini,1,Rob Froester
"[~gjreda], want to have a look at the PR?",Chris Riccomini,1,Greg Reda
"I'm searching for documentation related to how Airflow works with EMR. I'm struggling to find anything here: https://airflow.incubator.apache.org/integration.html#aws    My main question is, can Airflow create an EMR cluster and bring it back down like AWS Data Pipeline?    Thanks!    EDIT: Found some information here:     Spark, EMR:  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/example_dags/example_emr_job_flow_automatic_steps.py  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/example_dags/example_emr_job_flow_manual_steps.py  - (uses emr hooks, operators) https://docs.google.com/presentation/d/1NG1P86HRlX43qTVucCTOsFqIbCvYdOhq_np90VlbVRc/edit#slide=id.gd40eeee67_1_0  - (uses shells scripts to launch and terminate emr clusters) https://www.agari.com/automated-model-building-emr-spark-airflow/  - (use shell script to spark-submit on a local spark installation) https://blog.insightdatascience.com/scheduling-spark-jobs-with-airflow-4c66f3144660  - (installs spark on each airflow worker node and runs local spark jobs without use of spark submit) https://medium.com/@calvertmg/airflow-integrating-with-apache-spark-50a7704dcebd  - (alternative mozilla implementation for emr spark job) https://github.com/mozilla/telemetry-airflow/blob/master/dags/operators/emr_spark_operator.py    EMR:   - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/hooks/emr_hook.py  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/emr_create_job_flow_operator.py  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/emr_add_steps_operator.py  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/emr_terminate_job_flow_operator.py    Spark:  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/hooks/spark_submit_hook.py  - https://github.com/apache/incubator-airflow/blob/master/airflow/contrib/operators/spark_submit_operator.py",Al Johri,,
"[~jlowin] Thanks for the feedback. But I don't quite get it. There's `BaseExecutor` class that all executors derived from, it has a field `queued_tasks` that's a dictionary of tasks where the key is the task key and the value is a tuple of `(command, priority, queue, task_instance)`. During heartbeat we already [retrieve the task from the db. | https://github.com/apache/incubator-airflow/blob/f4c44146cd38afe21c517230bff71d33ab041246/airflow/executors/base_executor.py#L110]  We can pass to the `execute_async` method.    I have a question then: I write a custom executor. How can I reuse available Operators and their fields in the executor. Say I write executor for Mesos, I want to have access to Docker operator fields from there and transform them to an HTTP call. What would be the preferable way to do that in Airflow?",Alexandr Nikitin,,
"The Scheduler is an object that decides what tasks to run; the Executor is an object that decides where to run those tasks. So the Scheduler hands a TaskInstance to the Executor and says ""I want to run this task"". That's why the Executor has access to the TaskInstance when tasks are being queued. The Executor then extracts the TI's ""command"" (for example: airflow run my_task) and its sole job is to decide how to run that command. execute_async() is where that decision is made. The important thing is that the thing being run is the string command, not the TI object, and the signature of execute_async should reflect that.     The larger question here is why would an operator ever influence the executor? An Operator is a series of instructions that are designed to be run in a self-contained way, no matter what the executor is. An executor is a way of kicking off an Operator. There should be no dependency between them. To be explicit: an Operator must be able to run under any Executor.    However, I am not familiar with Mesos so unfortunately I don't know exactly what you're trying to do -- but have you looked at the existing MesosExecutor in airflow.contrib.executors? Is it insufficient for your objectives? If your operator requires special handling, then it sounds like maybe you want a MesosOperator that runs your docker command?",Chris Riccomini,,
"This can be done via the tree view by clicking on the failed task(s), and clicking ""clear"" with ""downstream"". Is this what you're looking for? It will allow the DAG to resume.",Chris Riccomini,1,Junwei Wang
[~criccomini] I was thinking that would it be helpful to have a cli support for that.. say {{ resume_dag }} which takes a execution_date as input and rerun only failed tasks?,Sumit Maheshwari,1,Chris Riccomini
What do you mean by expired schedules? You mean DagRuns that have finished? Which page in the UI are you referring to?,Chris Riccomini,1,Sumit Maheshwari
This sounds reasonable to me. Want to send a PR?,Chris Riccomini,1,Sumit Maheshwari
"Specifically, the CLI solution sounds reasonable. The REST solution is better, but we haven't yet set up a REST API for Airflow yet. In the mean time, want to send a PR for the CLI {{dag_state}} command?",Chris Riccomini,1,Sumit Maheshwari
"I think this is already done, right?    https://pythonhosted.org/airflow/concepts.html#task-documentation-notes    You can do:    {code}  t = BashOperator(""foo"", dag=dag)  t.doc_md = """"""\  #Title""  Here's a [url](www.airbnb.com)  """"""  {code}",Chris Riccomini,,
"What context is this under? You mean like you want to use the EmailOperator or SlackOperator to notify people to download a file that's been created as part of the DAG?    We do this using XCom+EmailOperator. XCom variables can be accessed via templates. We store the file in a blob store (like S3). The file location is stored in XCom, which the EmailOperator references when it sends the email.",Chris Riccomini,1,Sumit Maheshwari
"I don't think that we want to embed Quoble logic directly in Airflow. I'm a bit out of my element on the UI-front, though. Perhaps there's a way to achieve this through plugins, or by simply putting the link in the logs?",Chris Riccomini,1,Sumit Maheshwari
"Sure, can you please cc top contributors to PR or here.",Sumit Maheshwari,,
"I'm +1 on cutting branches for releases (instead of tags).    I'm also +1 on deleting dead branches, but could you post a list of the proposed branches to delete so we can allow people time to notify if they have a branch that they don't want deleted?",Chris Riccomini,,
"[~maxime.beauchemin@apache.org], [~aoen], [~artwr], could one of you take a look at this? I think you guys are the big Celery users.",Chris Riccomini,,
"[~bolke], OK to close this?",Chris Riccomini,1,Bolke de Bruin
What exactly do you define as a timeout?    # Task is killed because it's a zombie  # A connection to some infrastructure times out  # A task runs for longer than it should    (1) seems to be the one that Airflow could do right now.    (2) is hard--it requires cooperation from the operator to throw some kind of known TimeoutException (unless you want to use Python built-in TimeoutError).    (3) I supposed could be handled as a task-level SLA.,Chris Riccomini,1,Kevin Mandich
Hello.  I think you should file a new bug issue with an example DAG that triggers this bug. Please also specify which version you are running.,dud,1,flex gao
Was there a PR for this?,Chris Riccomini,1,Hongbo Zeng
[~artwr] Are you working on this issue? I'm facing many errors when using the current `S3Hook` mostly caused by it using `boto` instead of `boto3` (e.g. https://github.com/boto/boto/issues/2836),Vitor Baptista,,
"[~Shenghu], can you:    # Test this against master?  # Post your DAG (or some equivalent) Python code so we can try and replicate",Chris Riccomini,1,Shenghu Yang
"Also, I wonder if perhaps you want max_active_runs instead of concurrency?    From [FAQ|https://pythonhosted.org/airflow/faq.html]:     {quote}  concurency defines how many running task instances a DAG is allowed to have, beyond which point things get queued  {quote}    Whereas:    {quote}  Is the max_active_runs parameter of your DAG reached? max_active_runs defines how many running concurrent instances of a DAG there are allowed to be.  {quote}",Chris Riccomini,1,Shenghu Yang
Thanks [~criccomini]. I fixed the description by changing 'jobs' to 'task instances'.     Can you please help on review the related PR: https://github.com/apache/incubator-airflow/pull/1579 ?,Shenghu Yang,1,Bolke de Bruin
"[~bolke], which PR was this part of?",Chris Riccomini,,
"[~underyx] I'm rapidly learning about ExternalTaskSensor but could you not create 6 ETS in your hourly DAG, one for each of the 10 minute tasks (so one with delta of 10 minutes, one with 20 minutes, one with 30 minutes, etc.) and then have your hourly task depend on all of those sensors completing successfully? See for example AIRFLOW-47.",Jeremiah Lowin,1,Bence Nagy
"ok i pushed new PR https://github.com/apache/incubator-airflow/pull/1497 but i really misunderstand (if i replace  'ssl_certfile': None, section)  i got :  {color:red}  Traceback (most recent call last):    File ""/usr/local/bin/airflow"", line 13, in <module>      parser = get_parser()    File ""/usr/local/lib/python2.7/dist-packages/airflow/bin/cli.py"", line 689, in get_parser      default=configuration.get('webserver', 'ssl_keyfile'),    File ""/usr/local/lib/python2.7/dist-packages/airflow/configuration.py"", line 520, in get      return conf.get(section, key, **kwargs)    File ""/usr/local/lib/python2.7/dist-packages/airflow/configuration.py"", line 428, in get      ""in config"".format(**locals()))  airflow.configuration.AirflowConfigException: section/key [webserver/ssl_keyfile] not found in config  {color}  so please check ones more my changes ?  thanks !",Stanilovsky Evgeny,1,Chris Riccomini
"i update related issue, pls take a look ones more ?",Stanilovsky Evgeny,,
"hi for all, i already update PR, plz take a look on it? thanks !",Stanilovsky Evgeny,,
What do you think about writing a new sensor based off of the TimeSensor that blocks during the blackout period?,Chris Riccomini,,
Is this dead in the water? I've been reading about it in the pull request and on the mailing list and on the Gitter channel...,Jimbo Freedman,1,Chris Riccomini & thoralf gutierrez
[~abridgett]    Can you tell me more about your expectations of clear?,Siddharth Anand,1,Adrian Bridgett
Assigning this to Norman. We can clear dag runs issued by clear in both the UI and CLI.    [~bolke] [~jlowin][~maxime.beauchemin@apache.org] Do you see any issue with this idea?,Siddharth Anand,1,Maxime Beauchemin
"[~abridgett]: Just to clarify, the issue here is still that the cleared dagruns get rerun without respecting the max_active_runs amount?",Norman Mu,1,Adrian Bridgett
[~abridgett]Is max_active_runs in this scenario set to 1? Did you set it to depend on upstream? And nothing else is running?,Norman Mu,1,Maxime Beauchemin & Adrian Bridgett
"[~asnir], can you describe what this is a bit more?",Chris Riccomini,1,Amikam Snir
"[~asnir], do you want add config to the DockerOperator, such that it can execute a `docker login` before it runs the Docker command?",Chris Riccomini,1,Amikam Snir
"Hi [~criccomini],  I'm really needing it, and I think it's simple to do that...  I was thinking in: Expose the Client (docker-py.client.Client) that is instantiated inside of DockerOperator.execute() method, changing it to instantiate in __init__(), so we will be able to execute the login passing username, password, and the specific registry too....  eg:    d = DockerOperator(      task_id='example_task_id',      image='example/image-name',      force_pull=True,      owner='felipetancredo',      command=""./bi_jobs/etl/example.py {{ execution_date }} {{ execution_date }}""  )  user = 'test'  pwd='pwd'  if d.cli:      d.cli.login(username=user, password=pwd, registry='https://registry.example.com', reauth=True)    Can I contribute to docker_operator? Do u see any problem?  (sorry, I'm beginner in the open source projects)",Felipe Tancredo,1,Amikam Snir
When will [~asnir]'s PR be approved?  It's been 8 months.,Andrew Stewart,,
"[~maxime.beauchemin@apache.org], any reason why this was closed?",Chris Riccomini,,
"[~sanand], did you mean to put a real external URL in? You just put the JIRA in. ;)",Chris Riccomini,1,siddarth anand
"Overall, this looks good to me. Given that it's in contrib, and basically can't be supported by the committers (unless one of us wants to use it?), I'm OK without tests.    [~sanand], what's your take?",Siddharth Anand,1,siddarth anand
"""=+1 for this feature as well.     cc @jlowin (the original creator of xcom)    [~amustafa] Would you like to take this on?  The one case you need to think about is subdags. I think the relationships above translate across subdags, but please do test with subdags as well.    Also, please change '@upstream' to '@parent'""",Siddharth Anand,1,adam mustafa
[~amustafa]  Any PRs that we can look at?,Siddharth Anand,,
[~criccomini] Can we do an 'airbnb' release for now then? Ie. not use the apache brand? Cause this release is long overdue and 1.7.0 is a bit buggy. Chasing licenses will cost at least another week or even more (with broken PRs to add),,,
[~jghoman]/[~hitesh]/[~cnauroth] what's the best way to handle this? We basically had an RC in flight for like a month before Apache.,,,
"So, the release will be coordinated (and built from) github and github resources? I think Chris is right, but I'd be concerned about running the vote or whatever process was being used for the release on ASF resources.  I'd want to avoid the appearance of trying to make an end-run around asf procedure (which I know isn't the case).    I'm honestly not sure though; may be worth pinging general@incubator.",,,
"[~aoen], are you planning to burn in [~jlowin]'s fix before release?",,,
This one right https://github.com/apache/incubator-airflow/pull/1473 ? In this case yes.,,,
Should this be closed?,,,
"Shouldn't we support both, rather than deleting the `cpus` field? As I read it, it sounds like Docker supports both CPU count and cpu_shares percentage.",Chris Riccomini,1,Amikam Snir
"It's likely not implemented because no one is using it. Can you provide an example of your uri?    Is it something like *postgresql://localhost:2345/your_db?sslmode=require*?    What version of postgresql are you running? I'm not sure how well this parameter is supported for versions of postgresql before 9.2.    Another alternative is to take control of this by implementing this logic in a PythonOperator's python callable. That could read this param and use psycopg2 to connect to your db. Of course, then you would not be using the Connection or Hook, but, you would be able to achieve your end goal.     In the meantime, feel free to submit a PR for review -- we'd be happy to accept it.",Siddharth Anand,,
"What do you think about defining a heuristic that variables would be hidden from the UI if they contain the text: hidden, password, secret",Chris Riccomini,,
"{quote}  I think we should have a configuration variable to hide/expose the encrypted values in the UI for both Variables and Connections. It should likely affect all Variables and/or Connections in a given Airflow installation and not be DAG specific.  {quote}    This seems OK to me. So, current proposal is:    # Add a config param to airflow.cfg called hide_encrypted_ui_fields  # Default hide_encrypted_ui_fields to true    The hide_encrypted_ui_fields param would hide `passwords` and `extras` in the hooks view, as well as the `value` of variables in the variables view.    Does that sound OK?",Siddharth Anand,1,Siddharth Anand
"Sounds good. [~cheny258], would you like to try this?",Chris Riccomini,1,Matthew Chen
"[~cheny258] As part of this Variables work, there are currently 2 views. One is the edit view, the other a list view of all variables. Currently, the list view only shows the variables names, not the values. i'd like to display the values if we the configuration param proposed above is set to False. Can you add this feature?     Currently, one must click edit to view a variable value.",Siddharth Anand,1,Matthew Chen
"[~sanand] will do that for the list view of variables.  Also, for conn edit page, {{extra}} and {{password}} are already in a hidden_fields list. Do we want to make any changes in edit or list view for that?",Matthew Chen,1,Siddharth Anand
Could we not handle this with a regexp of fields that should be hidden rather than a boolean?,Paul Rhodes,1,Chris Riccomini
"{quote}  I'd have to go change the config whenever I do this, which is kind of annoying  {quote}    (Yes, quoting myself :)). Perhaps not. As long as the regex was something like '.*secret.*', and I stuck to that as a naming convention, I'd be fine. Still, to limit burden on [~cheny258], I'd like to keep the regex to a separate ticket. I'll open it up after this one is closed off. [~withnale]/[~sanand], does that sounds OK?",Chris Riccomini,,
Can you provide sample code?,Siddharth Anand,1,dud
"Oh, so not version 1.7.0? What does 'git log | head' look like for you?",Siddharth Anand,,
"[~sanand], any particular reason you closed this?",Chris Riccomini,1,Siddharth Anand
I'm not sure where the best place to comment is... Wherever you're comfortable? Let me know if you discover anything -- in my experience getting objects to behave like modules is tricky (supporting full {{import}} syntax etc). Maybe some of what I did for plugins would be reusable?,Chris Riccomini,,
"Initially, this seemed fine to me, but it actually sounds a little tricky to me. The concern I have is what happens if the DAG fails half way through and the transaction is reverted? In such a case, Airflow will show prior operators as having successfully run, but their state mutation will not have taken place. This effectively means that the entire DAG needs to be re-run, even if some of the operators show a successfully executed view.",Chris Riccomini,1,Bence Nagy
"In your use cases, are the source and destination DBs part of the same DB cluster?",Chris Riccomini,1,Bence Nagy
"One thing to consider: what happens if the scheduler machine is overloaded, and imports are taking a long time because of that? (similar to a GC pause in Java).",Chris Riccomini,1,Bence Nagy
"[~underyx], re (1), I wasn't suggesting GC would be an issue--was just saying it's a similar problem to timeouts in Java when GC occurs. Anyway, I agree, probably not worth worrying about. What default did you have in mind? 5s?",Chris Riccomini,1,Bence Nagy
"[~underyx] I'm not connecting the configuration suggestion to how such a change would resolve your problem. Do you mind providing a little more context on impact of such a change? If the import is actually only taking 1 second on your box and if you are not hitting a timeout, then why does the larger configured value have an impact on you?",Siddharth Anand,1,Bence Nagy
"Okay, but what's preventing someone else from implementing this? How come the issue was closed as won't fix?",Bence Nagy,1,siddarth anand
"Hey [~alexvanboxel], checking in to see how this and AIRFLOW-24 are coming.",Chris Riccomini,1,Alex Van Boxel
Can public buckets/BQ datasets (thinking about GDELT) be used without a project?,Jeremiah Lowin,1,Chris Riccomini
"What's GDELT?    In the case of GCS, yes I believe we can use public buckets. I'm uncomfortable baking in someone's own buckets, though. It'd have to be public buckets that Google, themselves, provide. For BQ, unfortunately not. The problem with BQ is that it requires a project to run its queries on. Even if the datasets are public, you still need to have access to a whole project, and be a viewer for it, in order to execute any queries against the data.",Chris Riccomini,1,Jeremiah Lowin
Do you need some help? I could do it.,Alex Van Boxel,1,Chris Riccomini
"[~alexvanboxel], that would be awesome. Can you wait until AIRFLOW-16 goes in, though, to avoid merge conflicts? Should be today.",Chris Riccomini,,
"The DagRun.previous handling is used to connect previous related DagRuns in order to make depends_on_past work with a moving interval or an automatically aligned start_date.    Imagine the following    Use Case 1  Dag1 has an assigned start_date ""2016-04-24 00:00:00"" with a cron schedule of “10 1 * * *”, ie run every day at 1.10am. Depends_on_past is true.    The current scheduler kicks off a run “2016-04-24 00:00:00” and then stops as the previous interval of “2016-04-24 01:10:00” is “2016-04-23 01:10:00”. This cannot be found as it has not taken place and thus the whole thing grinds to a halt and the TaskInstances refuse to run.    What the user expects here is that the first run is “2016-04-24 01:10:00”, ie start_date + interval, unless the start_date is on the interval, ie. start_date is first interval. This is what I address by start_date normalization in the PR. However, the second issue then kicks in as the “previous” run can still not be found.    Use Case 2  Dag2 has an assigned start_date ""2016-04-24 01:10:00""  with a cron schedule of “10 1 * * *”. Depends_on_past is true.    The scheduler happily goes through a few runs, but then the dag is updated and the schedule adjusted. Because the previous interval cannot be found by the TaskInstance (emphasis), tasks get stuck again requiring an manual override.    What the user expects here is that the scheduler is smart enough to figure out that we are still running the same dag and that it needs to look up the previous run for that dag and make sure dependencies are met with that previous dagrun in mind.    I don’t think those two use cases are edge cases, considering the amount of questions we get on these subjects.    To resolve the remaining issues (aside from start_date normalization) I first made DagRun aware of its predecessors. Then I strengthened the relationship between TaskInstances and DagRuns slightly, by optionally including a dagrun_id in the TaskInstance. Now a TaskInstance can lookup its predecessors in the previous DagRun and know for sure that it is either the first run or it has a predecessor somewhere in time instead of guessing.     What I am unsure of is what to consider is the unit of work: 1) is a TaskInstance dependent on its past predecessor ignoring the outcome of the DagRun? (previous TaskInstance can be successful, but previous DagRun as a whole can fail) or 2) is it dependent on the outcome of the DagRun? 3) Can it be both? In case of 1 and 3 my logic needs to be updated slightly, but that should not be too much of a big deal. However I have difficulty imagining why you want to do that.",Bolke de Bruin ,1,Maxime Beauchemin/Chris Riccomini 
"Max I understand your concerns, I'm working with Jeremiah to see if I can remove the need for the dag run id in taskinstances. I would be happy to as it would simplify the change. Rigorous testing needs to take place before making it part of a release, current unit tests do not cover enough.     Documentation is forthcoming but I will only assemble it when all assumptions have been worked out and discussions finished.     Please also note that we (Jeremiah and I) are seeing this as the first step towards his bigger pr. Sort of paving the way. If you can regard it in that context and not just ""making it easier for schedules that move"".     In that regard the more fundamental questions I have also asked on the list have not been answered yet: what do we consider the unit of work? Is it a DagRun or is it a taskinstance? We talk about dags, but we can run tasks without a DagRun.     I consider taskinstances part of a DagRun. They should not be able to look beyond the borders of the containing DagRun. If they want to do so they should query the DagRun for this. If you do this backfills and so on become much easier to handle.",Bolke de Bruin ,,
"I took the plunge and made the ""dag_run_id"" field for TaskInstances NOT NULL. This allowed me to catch places where TaskInstances are manipulated and to fix these. So indeed there are some strange places like the PythonOperator and views.py that create TaskInstances. That is really a refactor task towards the future (eg. dao principle would be smart maybe)    The good news is these have all been fixed. All tests pass, including some additional ones I added.     Now I can dive in to some other cases. Like what to do when a backfill has an execution_date that is equal to a previous scheduled run or backfill?     1. Do we update the tasks and run new ones and skip ones that were successful? If so do we connect old ones to the backfill run or do we leave them as is (ie connected to the old run). Think lineage here. How do we handle dependencies if the old tasks stay connected to their previous run?  2. Do we refuse to run? That would disallow you to ""add tasks"" in the past.    I tend to lean towards no 1 and connect all tasks to the backfill and setting the state of the ""old"" dagrun to ""SUPERSEDED"" or ""OVERRIDDEN"". The same flexibility as today would exist but an audit trail is preserved.",Bolke de Bruin ,,
Got it. Is there any way to fix it?,Chris Riccomini,1,Arthur Wiedmer & Bolke de Bruin
"Pull request up at: https://github.com/airbnb/airflow/pull/1452    This PR should also fix: https://github.com/airbnb/airflow/issues/1354    [~jlowin]/[~alexvanboxel], I could use a review from you.",Chris Riccomini,1,Jeremiah Lowin and Alex Van Boxel
"> Got it working with the new hooks    Did you find any bugs, or started up without incident?    > GCP operations and hooks be prefixed with gcp ... Could be a ticket, I'll pick it up (after this is merged).    Sounds good. Please do! Go ahead and open the JIRA.",Chris Riccomini,1,Alex Van Boxel
"Pull request: https://github.com/airbnb/airflow/pull/1448    [~jlowin], could use a review.",,,
[~criccomini] is this issue resolved?,Bolke de Bruin,1,Chris Riccomini
"Overall, major +1 on this.    One thing that I think is glossed over is what happens when a DagRunJob dies ({{kill -9}}, for example)? You mention heartbeats, but I don't see a mention of who is going to clean up/take over the execution of the wedged DagRuns when their job dies. I see this problem a lot with Airflow right now (DagRuns stuck in 'running' forever), so it'd be good to think through.",Chris Riccomini,1,Jeremiah Lowin 
"If I'm not mistaken, I think this work should solidify running multiple schedulers as a first-class citizen, right? We are not going to run Celery for distributed execution, so having a trustworthy way to run more than one scheduler is useful. /cc [~sanand]",Chris Riccomini,1,Jeremiah Lowin 
"How does a DRJ grab a Dag to execute? It seems to me like there's a race condition where two DRJs could simultaneously see that a DAG needs to be run, and both could insert entries into the dag_run table simultaneously. Is this being enforced by a primary key/unique constraint in the DB?",Chris Riccomini,1,Jeremiah Lowin 
"> DRJ needs to make sure to refresh from the db to check for a lock immediately before running it    Isn't there a race condition where two DRJs check the lock in the DB before running, both see it's unset, and both set it to themselves? Even if they then check to verify that they are the owners of the lock, it's not trustworthy. The only way I can see this working is if a real read/write lock is obtained. Is SQLAlchemy doing this?",Chris Riccomini,1,Jeremiah Lowin 
"The good news is that lock_id is already set to the job's id, so I just need to add the additional check that it matches (right now it just assumes if it was able to lock, then it owns the lock, which is not as strict). But I'm still curious about your point:    > Even if they then check to verify that they are the owners of the lock, it's not trustworthy.    Is that not still an issue?",Jeremiah Lowin ,,
"To clarify, would different DagRuns be evaluated in the same process, or in different processes? In the current work-in-progress branch, I noticed that task scheduling is done by each dag run. However, with the current approach, pool limits may not be correctly respected?",Paul Yang,1,Chris Riccomini
"While I understand that for being truly distributed it would be great to have a distributed scheduler, are not not looking for a distributed executor instead [~criccomini]? Now the scheduler and the executor are coupled, but that does not need to be and does not even make sense imho.",Bolke de Bruin,1,Chris Riccomini
{quote}  Now the scheduler and the executor are coupled  {quote}    Can you expand on this a bit? I know [~sanand] is running multiple LocalExecutor schedulers. We were planning to do this as well.,Chris Riccomini,,
"Now the scheduler is, in case of using the LocalExecutor, tied to the executor: ie. both are same process. This is nor really required and maybe not even the best way forward. A scheduler should, imho, only schedule and an executor should execute. With celery you already get a kind of decoupling. The celery executor puts tasks on a queue and then celery workers pick these up. It is not so difficult to imagine that the local executor can do the same and pick up tasks for a queue that is either something like rabbitmq or just the db.    In other words the scheduler should queue tasks in a certain order and the executor should pick these up from anuwhere. Ie. producer/consumer like.    Does this make sense?Bolke de Bruin ",Bolke de Bruin ,1,Chris Riccomini
"Yea, I agree with this. I guess the two things that I'm worried about are:    # Will this mean we can't safely run more than one LocalExecutor?  # Will this break the CeleryExecutor?",,,